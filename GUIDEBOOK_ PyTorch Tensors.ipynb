{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["DCJa2cuHC3um","W-DnEIroDDzd","6G7Z8Ht6DDz0"],"toc_visible":true,"authorship_tag":"ABX9TyPk90gRpe79+EJjchwtANkq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JxXlKp4hC3uh"},"source":["<img src=\"../Pierian-Data-Logo.PNG\">\n","<br>\n","<strong><center>Copyright 2019. Created by Jose Marcial Portilla.</center></strong>"]},{"cell_type":"markdown","metadata":{"id":"DCJa2cuHC3um"},"source":["# Tensor Basics\n","This section covers:\n","* Converting NumPy arrays to PyTorch tensors\n","* Creating tensors from scratch\n","\n","## Perform standard imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PwI4_3NbC3un"},"outputs":[],"source":["import torch\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"jZikUBm8C3uo"},"source":["Confirm you're using PyTorch version 1.1.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cGDnM5RBC3up","outputId":"eb528bb6-1b02-4b72-f9c4-1374d7c1cf19"},"outputs":[{"data":{"text/plain":["'1.1.0'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["torch.__version__"]},{"cell_type":"markdown","metadata":{"id":"oePo3hVgC3ur"},"source":["## Converting NumPy arrays to PyTorch tensors\n","A <a href='https://pytorch.org/docs/stable/tensors.html'><strong><tt>torch.Tensor</tt></strong></a> is a multi-dimensional matrix containing elements of a single data type.<br>\n","Calculations between tensors can only happen if the tensors share the same dtype.<br>\n","In some cases tensors are used as a replacement for NumPy to use the power of GPUs (more on this later)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"REetTFxeC3us","outputId":"52cc44bd-6478-43fd-a280-ab13174b3893"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1 2 3 4 5]\n","int32\n","<class 'numpy.ndarray'>\n"]}],"source":["arr = np.array([1,2,3,4,5])\n","print(arr)\n","print(arr.dtype)\n","print(type(arr))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VjSHBWRSC3us","outputId":"91e5e7a0-c3c2-49e6-bd78-354cb26ec62a"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1, 2, 3, 4, 5], dtype=torch.int32)\n"]}],"source":["x = torch.from_numpy(arr)\n","# Equivalent to x = torch.as_tensor(arr)\n","\n","print(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZaWcs6HC3ut","outputId":"3e409880-0a47-465e-fdea-2301b0928392"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.int32\n"]}],"source":["# Print the type of data held by the tensor\n","print(x.dtype)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kxlU73HtC3uu","outputId":"6746702a-58ce-46fc-ed83-67ff5dc45f39"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'torch.Tensor'>\n","torch.IntTensor\n"]}],"source":["# Print the tensor object type\n","print(type(x))\n","print(x.type()) # this is more specific!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eW1TsA2MC3uu","outputId":"8f52a2e3-e696-4b82-b11d-f3245dd9579a"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 0.  1.  2.]\n"," [ 3.  4.  5.]\n"," [ 6.  7.  8.]\n"," [ 9. 10. 11.]]\n"]}],"source":["arr2 = np.arange(0.,12.).reshape(4,3)\n","print(arr2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vxL7CNr4C3uv","outputId":"a315443b-9933-404b-faa1-30bb06fd6450"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.,  1.,  2.],\n","        [ 3.,  4.,  5.],\n","        [ 6.,  7.,  8.],\n","        [ 9., 10., 11.]], dtype=torch.float64)\n","torch.DoubleTensor\n"]}],"source":["x2 = torch.from_numpy(arr2)\n","print(x2)\n","print(x2.type())"]},{"cell_type":"markdown","metadata":{"id":"xrZzBW5AC3uw"},"source":["Here <tt>torch.DoubleTensor</tt> refers to 64-bit floating point data."]},{"cell_type":"markdown","metadata":{"id":"Z5c5U9__C3ux"},"source":["<h2><a href='https://pytorch.org/docs/stable/tensors.html'>Tensor Datatypes</a></h2>\n","<table style=\"display: inline-block\">\n","<tr><th>TYPE</th><th>NAME</th><th>EQUIVALENT</th><th>TENSOR TYPE</th></tr>\n","<tr><td>32-bit integer (signed)</td><td>torch.int32</td><td>torch.int</td><td>IntTensor</td></tr>\n","<tr><td>64-bit integer (signed)</td><td>torch.int64</td><td>torch.long</td><td>LongTensor</td></tr>\n","<tr><td>16-bit integer (signed)</td><td>torch.int16</td><td>torch.short</td><td>ShortTensor</td></tr>\n","<tr><td>32-bit floating point</td><td>torch.float32</td><td>torch.float</td><td>FloatTensor</td></tr>\n","<tr><td>64-bit floating point</td><td>torch.float64</td><td>torch.double</td><td>DoubleTensor</td></tr>\n","<tr><td>16-bit floating point</td><td>torch.float16</td><td>torch.half</td><td>HalfTensor</td></tr>\n","<tr><td>8-bit integer (signed)</td><td>torch.int8</td><td></td><td>CharTensor</td></tr>\n","<tr><td>8-bit integer (unsigned)</td><td>torch.uint8</td><td></td><td>ByteTensor</td></tr></table>"]},{"cell_type":"markdown","metadata":{"id":"jZ8NMd3rC3ux"},"source":["## Copying vs. sharing"]},{"cell_type":"markdown","metadata":{"id":"YDrDluKBC3uy"},"source":["<a href='https://pytorch.org/docs/stable/torch.html#torch.from_numpy'><strong><tt>torch.from_numpy()</tt></strong></a><br>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.as_tensor'><strong><tt>torch.as_tensor()</tt></strong></a><br>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.tensor'><strong><tt>torch.tensor()</tt></strong></a><br>\n","\n","There are a number of different functions available for <a href='https://pytorch.org/docs/stable/torch.html#creation-ops'>creating tensors</a>. When using <a href='https://pytorch.org/docs/stable/torch.html#torch.from_numpy'><strong><tt>torch.from_numpy()</tt></strong></a> and <a href='https://pytorch.org/docs/stable/torch.html#torch.as_tensor'><strong><tt>torch.as_tensor()</tt></strong></a>, the PyTorch tensor and the source NumPy array share the same memory. This means that changes to one affect the other. However, the <a href='https://pytorch.org/docs/stable/torch.html#torch.tensor'><strong><tt>torch.tensor()</tt></strong></a> function always makes a copy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ozfWO6KTC3uz","outputId":"75e633aa-7446-4a04-d3e2-23469e32373b"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0, 1, 2, 3, 4], dtype=torch.int32)\n"]}],"source":["# Using torch.from_numpy()\n","arr = np.arange(0,5)\n","t = torch.from_numpy(arr)\n","print(t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_IdjFxV-C3uz","outputId":"8d9800b7-edc8-431a-bbff-ce18b1908365"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([ 0,  1, 77,  3,  4], dtype=torch.int32)\n"]}],"source":["arr[2]=77\n","print(t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ic-k3qCjC3u0","outputId":"7b803596-6ed6-46e9-dbd9-d97be77e389e"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0, 1, 2, 3, 4], dtype=torch.int32)\n"]}],"source":["# Using torch.tensor()\n","arr = np.arange(0,5)\n","t = torch.tensor(arr)\n","print(t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l5qpRzr8C3u1","outputId":"04318154-9402-42b0-e224-49d1ddeae7ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0, 1, 2, 3, 4], dtype=torch.int32)\n"]}],"source":["arr[2]=77\n","print(t)"]},{"cell_type":"markdown","metadata":{"id":"y_fY10fDC3u1"},"source":["## Class constructors\n","<a href='https://pytorch.org/docs/stable/tensors.html'><strong><tt>torch.Tensor()</tt></strong></a><br>\n","<a href='https://pytorch.org/docs/stable/tensors.html'><strong><tt>torch.FloatTensor()</tt></strong></a><br>\n","<a href='https://pytorch.org/docs/stable/tensors.html'><strong><tt>torch.LongTensor()</tt></strong></a>, etc.<br>\n","\n","There's a subtle difference between using the factory function <font color=black><tt>torch.tensor(data)</tt></font> and the class constructor <font color=black><tt>torch.Tensor(data)</tt></font>.<br>\n","The factory function determines the dtype from the incoming data, or from a passed-in dtype argument.<br>\n","The class constructor <tt>torch.Tensor()</tt>is simply an alias for <tt>torch.FloatTensor(data)</tt>. Consider the following:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gfJXfdZEC3u1"},"outputs":[],"source":["data = np.array([1,2,3])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_vOleaF9C3u2","outputId":"6bbf9ff1-1428-4b18-cd50-269c12c91db9"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1., 2., 3.]) torch.FloatTensor\n"]}],"source":["a = torch.Tensor(data)  # Equivalent to cc = torch.FloatTensor(data)\n","print(a, a.type())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LK_CXCY1C3u2","outputId":"d8d1f017-a112-449c-9d6b-8e2c83a6524c"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1, 2, 3], dtype=torch.int32) torch.IntTensor\n"]}],"source":["b = torch.tensor(data)\n","print(b, b.type())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VPx4mxI1C3u2","outputId":"fc7208b6-dde9-4400-eff4-3259af8c219d"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1, 2, 3]) torch.LongTensor\n"]}],"source":["c = torch.tensor(data, dtype=torch.long)\n","print(c, c.type())"]},{"cell_type":"markdown","metadata":{"id":"nsPlfw5JC3u3"},"source":["## Creating tensors from scratch\n","### Uninitialized tensors with <tt>.empty()</tt>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.empty'><strong><tt>torch.empty()</tt></strong></a> returns an <em>uninitialized</em> tensor. Essentially a block of memory is allocated according to the size of the tensor, and any values already sitting in the block are returned. This is similar to the behavior of <tt>numpy.empty()</tt>."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J7p6BhsBC3u3","outputId":"44b43a93-d34c-4236-c923-26b3d8bcb59e"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.]])\n"]}],"source":["x = torch.empty(4, 3)\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"E2EqBLDDC3u4"},"source":["### Initialized tensors with <tt>.zeros()</tt> and <tt>.ones()</tt>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.zeros'><strong><tt>torch.zeros(size)</tt></strong></a><br>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.ones'><strong><tt>torch.ones(size)</tt></strong></a><br>\n","It's a good idea to pass in the intended dtype."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1X7ZqlalC3u4","outputId":"18be0f37-5ecb-4621-804b-91e963c00347"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]])\n"]}],"source":["x = torch.zeros(4, 3, dtype=torch.int64)\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"omBDp-hHC3u4"},"source":["### Tensors from ranges\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.arange'><strong><tt>torch.arange(start,end,step)</tt></strong></a><br>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.linspace'><strong><tt>torch.linspace(start,end,steps)</tt></strong></a><br>\n","Note that with <tt>.arange()</tt>, <tt>end</tt> is exclusive, while with <tt>linspace()</tt>, <tt>end</tt> is inclusive."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5VBzoXW6C3u5","outputId":"6acb18e5-ab3c-452f-d2e1-161302b88153"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0,  2,  4],\n","        [ 6,  8, 10],\n","        [12, 14, 16]])\n"]}],"source":["x = torch.arange(0,18,2).reshape(3,3)\n","print(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s6Q3mZVsC3u5","outputId":"6fa707af-18f0-4f50-a009-7f2b8da0576b"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.0000,  1.6364,  3.2727,  4.9091],\n","        [ 6.5455,  8.1818,  9.8182, 11.4545],\n","        [13.0909, 14.7273, 16.3636, 18.0000]])\n"]}],"source":["x = torch.linspace(0,18,12).reshape(3,4)\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"-zvxULNQC3u5"},"source":["### Tensors from data\n","<tt>torch.tensor()</tt> will choose the dtype based on incoming data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nvvtWBWfC3u5","outputId":"fc13515b-f1c6-40e7-9e04-ab7d3f3fa96f"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1, 2, 3, 4])\n","torch.int64\n","torch.LongTensor\n"]}],"source":["x = torch.tensor([1, 2, 3, 4])\n","print(x)\n","print(x.dtype)\n","print(x.type())"]},{"cell_type":"markdown","metadata":{"id":"N6GvDfQ8C3u6"},"source":["Alternatively you can set the type by the tensor method used.\n","For a list of tensor types visit https://pytorch.org/docs/stable/tensors.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4XhKK2DKC3u6","outputId":"90e29eb0-b9a4-4ac1-c413-2ff9e5586cbd"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([5., 6., 7.])\n","torch.float32\n","torch.FloatTensor\n"]}],"source":["x = torch.FloatTensor([5,6,7])\n","print(x)\n","print(x.dtype)\n","print(x.type())"]},{"cell_type":"markdown","metadata":{"id":"GSvTKBlNC3u6"},"source":["You can also pass the dtype in as an argument. For a list of dtypes visit https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"70-X_2qxC3u6","outputId":"26085f80-3ef9-4607-b195-176eb8376621"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([ 8,  9, -3], dtype=torch.int32)\n","torch.int32\n","torch.IntTensor\n"]}],"source":["x = torch.tensor([8,9,-3], dtype=torch.int)\n","print(x)\n","print(x.dtype)\n","print(x.type())"]},{"cell_type":"markdown","metadata":{"id":"E1vrdcHlC3u6"},"source":["### Changing the dtype of existing tensors\n","Don't be tempted to use <tt>x = torch.tensor(x, dtype=torch.type)</tt> as it will raise an error about improper use of tensor cloning.<br>\n","Instead, use the tensor <tt>.type()</tt> method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MnGcNe0SC3u7","outputId":"34f69f2f-0179-4c0d-af9b-3de61733730a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Old: torch.IntTensor\n","New: torch.LongTensor\n"]}],"source":["print('Old:', x.type())\n","\n","x = x.type(torch.int64)\n","\n","print('New:', x.type())"]},{"cell_type":"markdown","metadata":{"id":"-G7W-4bzC3u7"},"source":["### Random number tensors\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.rand'><strong><tt>torch.rand(size)</tt></strong></a> returns random samples from a uniform distribution over [0, 1)<br>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.randn'><strong><tt>torch.randn(size)</tt></strong></a> returns samples from the \"standard normal\" distribution [σ = 1]<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;Unlike <tt>rand</tt> which is uniform, values closer to zero are more likely to appear.<br>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.randint'><strong><tt>torch.randint(low,high,size)</tt></strong></a> returns random integers from low (inclusive) to high (exclusive)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A9IAIfhwC3u7","outputId":"a79b356d-5794-4cce-b40c-cb23b781bb07"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.0211, 0.2336, 0.6775],\n","        [0.4790, 0.5132, 0.9878],\n","        [0.7552, 0.0789, 0.1860],\n","        [0.6712, 0.1564, 0.3753]])\n"]}],"source":["x = torch.rand(4, 3)\n","print(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qTFWTRf0C3u8","outputId":"38355c9c-3381-4bda-ce95-f437b1f3b2fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.7164, -0.1538, -0.9980],\n","        [-1.8252,  1.1863, -0.1523],\n","        [ 1.4093, -0.0212, -1.5598],\n","        [ 0.1831, -0.6961,  1.3497]])\n"]}],"source":["x = torch.randn(4, 3)\n","print(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"spBdtVYOC3u8","outputId":"b214f571-5fed-4422-fd20-1b6182ba3ac1"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0, 3, 0],\n","        [1, 3, 4],\n","        [1, 2, 3],\n","        [4, 4, 3]])\n"]}],"source":["x = torch.randint(0, 5, (4, 3))\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"I-F8l28mC3u8"},"source":["### Random number tensors that follow the input size\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.rand_like'><strong><tt>torch.rand_like(input)</tt></strong></a><br>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.randn_like'><strong><tt>torch.randn_like(input)</tt></strong></a><br>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.randint_like'><strong><tt>torch.randint_like(input,low,high)</tt></strong></a><br> these return random number tensors with the same size as <tt>input</tt>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yy3VNysbC3u8","outputId":"5715de0f-cd86-4de8-9a6c-6a38e84c6d1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0.]])\n"]}],"source":["x = torch.zeros(2,5)\n","print(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oCKDPhtxC3u9","outputId":"39e76c00-3010-4a13-99fe-12c73f3720ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-0.5442, -0.3149,  0.0922,  1.1829, -0.7873],\n","        [ 0.3143,  0.9465,  0.4534,  0.4623,  2.2044]])\n"]}],"source":["x2 = torch.randn_like(x)\n","print(x2)"]},{"cell_type":"markdown","metadata":{"id":"CmnPLekGC3u9"},"source":["The same syntax can be used with<br>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.zeros_like'><strong><tt>torch.zeros_like(input)</tt></strong></a><br>\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.ones_like'><strong><tt>torch.ones_like(input)</tt></strong></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GQvSGrLQC3u9","outputId":"4efed481-1f93-41e2-e597-50271297e86a"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.]])\n"]}],"source":["x3 = torch.ones_like(x2)\n","print(x3)"]},{"cell_type":"markdown","metadata":{"id":"jb38sHN3C3u9"},"source":["### Setting the random seed\n","<a href='https://pytorch.org/docs/stable/torch.html#torch.manual_seed'><strong><tt>torch.manual_seed(int)</tt></strong></a> is used to obtain reproducible results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fZp01BjyC3u9","outputId":"9706e567-98e4-483e-ffcb-539e253f7657"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.8823, 0.9150, 0.3829],\n","        [0.9593, 0.3904, 0.6009]])\n"]}],"source":["torch.manual_seed(42)\n","x = torch.rand(2, 3)\n","print(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJiQvU0BC3u-","outputId":"b0660ea6-9ced-4fdc-95a0-9035790b4008"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.8823, 0.9150, 0.3829],\n","        [0.9593, 0.3904, 0.6009]])\n"]}],"source":["torch.manual_seed(42)\n","x = torch.rand(2, 3)\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"oXdBdCAzC3u-"},"source":["## Tensor attributes\n","Besides <tt>dtype</tt>, we can look at other <a href='https://pytorch.org/docs/stable/tensor_attributes.html'>tensor attributes</a> like <tt>shape</tt>, <tt>device</tt> and <tt>layout</tt>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EN6NgjCdC3u-","outputId":"c088536c-8e07-44fa-cf8b-9e778a48a2d0"},"outputs":[{"data":{"text/plain":["torch.Size([2, 3])"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["x.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xTQWg1SzC3u-","outputId":"e58bf9cb-7d1a-4621-fc33-9bdaba23fe86"},"outputs":[{"data":{"text/plain":["torch.Size([2, 3])"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["x.size()  # equivalent to x.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B0Sa7Is_C3u_","outputId":"e3d87a9d-f3cb-4980-ba80-87a9dfe9c32a"},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["x.device"]},{"cell_type":"markdown","metadata":{"id":"re7dxKMjC3u_"},"source":["PyTorch supports use of multiple <a href='https://pytorch.org/docs/stable/tensor_attributes.html#torch-device'>devices</a>, harnessing the power of one or more GPUs in addition to the CPU. We won't explore that here, but you should know that operations between tensors can only happen for tensors installed on the same device."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"2RijRW-gC3u_","outputId":"03f29b71-2a0c-4d34-ddfc-0e59d0e55cdb"},"outputs":[{"data":{"text/plain":["torch.strided"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["x.layout"]},{"cell_type":"markdown","metadata":{"id":"PnEu-ODhC3u_"},"source":["PyTorch has a class to hold the <a href='https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.layout'>memory layout</a> option. The default setting of <a href='https://en.wikipedia.org/wiki/Stride_of_an_array'>strided</a> will suit our purposes throughout the course."]},{"cell_type":"markdown","metadata":{"id":"4TMYECp1C3u_"},"source":["___"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"W-DnEIroDDzd"},"source":["# Tensor Operations\n","This section covers:\n","* Indexing and slicing\n","* Reshaping tensors (tensor views)\n","* Tensor arithmetic and basic operations\n","* Dot products\n","* Matrix multiplication\n","* Additional, more advanced operations\n","\n","## Perform standard imports"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"NvntFJnGDDze"},"outputs":[],"source":["import torch\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"CYjugGw2DDzg"},"source":["## Indexing and slicing\n","Extracting specific values from a tensor works just the same as with NumPy arrays<br>\n","<img src='../Images/arrayslicing.png' width=\"500\" style=\"display: inline-block\"><br><br>\n","Image source: http://www.scipy-lectures.org/_images/numpy_indexing.png"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"lRrdd4rODDzh","outputId":"9ddbe596-aaca-4ee8-d519-ccda5965ca0c"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0, 1],\n","        [2, 3],\n","        [4, 5]])\n"]}],"source":["x = torch.arange(6).reshape(3,2)\n","print(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gl0obA5DDDzi","outputId":"33c968e0-d683-4e64-d89f-adb231332211"},"outputs":[{"data":{"text/plain":["tensor([1, 3, 5])"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Grabbing the right hand column values\n","x[:,1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SPjm-g87DDzj","outputId":"fdd29ad5-be57-49e3-d435-c3a81003c549"},"outputs":[{"data":{"text/plain":["tensor([[1],\n","        [3],\n","        [5]])"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Grabbing the right hand column as a (3,1) slice\n","x[:,1:]"]},{"cell_type":"markdown","metadata":{"id":"u3yv96hoDDzj"},"source":["## Reshape tensors with <tt>.view()</tt>\n","<a href='https://pytorch.org/docs/master/tensors.html#torch.Tensor.view'><strong><tt>view()</tt></strong></a> and <a href='https://pytorch.org/docs/master/torch.html#torch.reshape'><strong><tt>reshape()</tt></strong></a> do essentially the same thing by returning a reshaped tensor without changing the original tensor in place.<br>\n","There's a good discussion of the differences <a href='https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch'>here</a>."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ylWtKtd9DDzl","outputId":"af2e82f9-7b69-4b07-998d-e390e40895c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"]}],"source":["x = torch.arange(10)\n","print(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDJ8jVR0DDzm","outputId":"d2ef94b2-1059-432f-a0a5-dc497fa2ddf8"},"outputs":[{"data":{"text/plain":["tensor([[0, 1, 2, 3, 4],\n","        [5, 6, 7, 8, 9]])"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["x.view(2,5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qz-iWt4aDDzn","outputId":"53cacd93-7e28-47af-f00c-f0691c93e9b0"},"outputs":[{"data":{"text/plain":["tensor([[0, 1],\n","        [2, 3],\n","        [4, 5],\n","        [6, 7],\n","        [8, 9]])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["x.view(5,2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UJa6uonFDDzn","outputId":"e5284b93-6a69-4e5e-bb8c-b25cc0417585"},"outputs":[{"data":{"text/plain":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# x is unchanged\n","x"]},{"cell_type":"markdown","metadata":{"id":"8KG9THEhDDzo"},"source":["### Views reflect the most current data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d0edoS_lDDzo","outputId":"4e09acdb-81d3-4cb0-de61-4465783942e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[234,   1,   2,   3,   4],\n","        [  5,   6,   7,   8,   9]])\n"]}],"source":["z = x.view(2,5)\n","x[0]=234\n","print(z)"]},{"cell_type":"markdown","metadata":{"id":"7l2VyyG2DDzp"},"source":["### Views can infer the correct size\n","By passing in <tt>-1</tt> PyTorch will infer the correct value from the given tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MF_QDcpuDDzp","outputId":"285ec3d5-92e6-434c-aaa7-c425c77dad8f"},"outputs":[{"data":{"text/plain":["tensor([[234,   1,   2,   3,   4],\n","        [  5,   6,   7,   8,   9]])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["x.view(2,-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t9li6-fODDzq","outputId":"64c1d463-9196-46e6-a370-d27e28bd5e3c"},"outputs":[{"data":{"text/plain":["tensor([[234,   1,   2,   3,   4],\n","        [  5,   6,   7,   8,   9]])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["x.view(-1,5)"]},{"cell_type":"markdown","metadata":{"id":"JeOB4ss6DDzr"},"source":["### Adopt another tensor's shape with <tt>.view_as()</tt>\n","<a href='https://pytorch.org/docs/master/tensors.html#torch.Tensor.view_as'><strong><tt>view_as(input)</tt></strong></a> only works with tensors that have the same number of elements."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gMAVXgnbDDzr","outputId":"8308af67-4896-4977-e8ad-042788faaf7e"},"outputs":[{"data":{"text/plain":["tensor([[234,   1,   2,   3,   4],\n","        [  5,   6,   7,   8,   9]])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["x.view_as(z)"]},{"cell_type":"markdown","metadata":{"id":"OBDCAJ59DDzr"},"source":["## Tensor Arithmetic\n","Adding tensors can be performed a few different ways depending on the desired result.<br>\n","\n","As a simple expression:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O17uPO9CDDzs","outputId":"ef52d63c-da3b-46c5-fb69-5245567284cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([5., 7., 9.])\n"]}],"source":["a = torch.tensor([1,2,3], dtype=torch.float)\n","b = torch.tensor([4,5,6], dtype=torch.float)\n","print(a + b)"]},{"cell_type":"markdown","metadata":{"id":"JHMdwtAhDDzs"},"source":["As arguments passed into a torch operation:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gfDt2Su1DDzs","outputId":"7286b70f-6295-4519-a48e-04359d0c59f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([5., 7., 9.])\n"]}],"source":["print(torch.add(a, b))"]},{"cell_type":"markdown","metadata":{"id":"fGtl1I3yDDzt"},"source":["With an output tensor passed in as an argument:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"32tX_h61DDzu","outputId":"086591b9-6364-4f32-f500-85e9bfccd912"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([5., 7., 9.])\n"]}],"source":["result = torch.empty(3)\n","torch.add(a, b, out=result)  # equivalent to result=torch.add(a,b)\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"uvU_laj1DDzu"},"source":["Changing a tensor in-place"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5nMjSKQDDzu","outputId":"d00390e6-7549-48bc-f7a0-498f4ea5872e"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([5., 7., 9.])\n"]}],"source":["a.add_(b)  # equivalent to a=torch.add(a,b)\n","print(a)"]},{"cell_type":"markdown","metadata":{"id":"xmydxXDiDDzv"},"source":["<div class=\"alert alert-info\"><strong>NOTE:</strong> Any operation that changes a tensor in-place is post-fixed with an underscore _.\n","    <br>In the above example: <tt>a.add_(b)</tt> changed <tt>a</tt>.</div>"]},{"cell_type":"markdown","metadata":{"id":"SnNRRSlpDDzv"},"source":["### Basic Tensor Operations\n","<table style=\"display: inline-block\">\n","<caption style=\"text-align: center\"><strong>Arithmetic</strong></caption>\n","<tr><th>OPERATION</th><th>FUNCTION</th><th>DESCRIPTION</th></tr>\n","<tr><td>a + b</td><td>a.add(b)</td><td>element wise addition</td></tr>\n","<tr><td>a - b</td><td>a.sub(b)</td><td>subtraction</td></tr>\n","<tr><td>a * b</td><td>a.mul(b)</td><td>multiplication</td></tr>\n","<tr><td>a / b</td><td>a.div(b)</td><td>division</td></tr>\n","<tr><td>a % b</td><td>a.fmod(b)</td><td>modulo (remainder after division)</td></tr>\n","<tr><td>a<sup>b</sup></td><td>a.pow(b)</td><td>power</td></tr>\n","<tr><td>&nbsp;</td><td></td><td></td></tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"yJQgsYrxDDzv"},"source":["<table style=\"display: inline-block\">\n","<caption style=\"text-align: center\"><strong>Monomial Operations</strong></caption>\n","<tr><th>OPERATION</th><th>FUNCTION</th><th>DESCRIPTION</th></tr>\n","<tr><td>|a|</td><td>torch.abs(a)</td><td>absolute value</td></tr>\n","<tr><td>1/a</td><td>torch.reciprocal(a)</td><td>reciprocal</td></tr>\n","<tr><td>$\\sqrt{a}$</td><td>torch.sqrt(a)</td><td>square root</td></tr>\n","<tr><td>log(a)</td><td>torch.log(a)</td><td>natural log</td></tr>\n","<tr><td>e<sup>a</sup></td><td>torch.exp(a)</td><td>exponential</td></tr>\n","<tr><td>12.34  ==>  12.</td><td>torch.trunc(a)</td><td>truncated integer</td></tr>\n","<tr><td>12.34  ==>  0.34</td><td>torch.frac(a)</td><td>fractional component</td></tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"GhIw4r__DDzv"},"source":["<table style=\"display: inline-block\">\n","<caption style=\"text-align: center\"><strong>Trigonometry</strong></caption>\n","<tr><th>OPERATION</th><th>FUNCTION</th><th>DESCRIPTION</th></tr>\n","<tr><td>sin(a)</td><td>torch.sin(a)</td><td>sine</td></tr>\n","<tr><td>cos(a)</td><td>torch.sin(a)</td><td>cosine</td></tr>\n","<tr><td>tan(a)</td><td>torch.sin(a)</td><td>tangent</td></tr>\n","<tr><td>arcsin(a)</td><td>torch.asin(a)</td><td>arc sine</td></tr>\n","<tr><td>arccos(a)</td><td>torch.acos(a)</td><td>arc cosine</td></tr>\n","<tr><td>arctan(a)</td><td>torch.atan(a)</td><td>arc tangent</td></tr>\n","<tr><td>sinh(a)</td><td>torch.sinh(a)</td><td>hyperbolic sine</td></tr>\n","<tr><td>cosh(a)</td><td>torch.cosh(a)</td><td>hyperbolic cosine</td></tr>\n","<tr><td>tanh(a)</td><td>torch.tanh(a)</td><td>hyperbolic tangent</td></tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"8oXyPSnUDDzw"},"source":["<table style=\"display: inline-block\">\n","<caption style=\"text-align: center\"><strong>Summary Statistics</strong></caption>\n","<tr><th>OPERATION</th><th>FUNCTION</th><th>DESCRIPTION</th></tr>\n","<tr><td>$\\sum a$</td><td>torch.sum(a)</td><td>sum</td></tr>\n","<tr><td>$\\bar a$</td><td>torch.mean(a)</td><td>mean</td></tr>\n","<tr><td>a<sub>max</sub></td><td>torch.max(a)</td><td>maximum</td></tr>\n","<tr><td>a<sub>min</sub></td><td>torch.min(a)</td><td>minimum</td></tr>\n","<tr><td colspan=\"3\">torch.max(a,b) returns a tensor of size a<br>containing the element wise max between a and b</td></tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"g7JKD7GPDDzw"},"source":["<div class=\"alert alert-info\"><strong>NOTE:</strong> Most arithmetic operations require float values. Those that do work with integers return integer tensors.<br>\n","For example, <tt>torch.div(a,b)</tt> performs floor division (truncates the decimal) for integer types, and classic division for floats.</div>"]},{"cell_type":"markdown","metadata":{"id":"U79IhbPrDDzw"},"source":["#### Use the space below to experiment with different operations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ByAHAWQoDDzw","outputId":"f4aa0a9c-806c-4b46-e98c-d99e2161cab6"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(21.)\n"]}],"source":["a = torch.tensor([1,2,3], dtype=torch.float)\n","b = torch.tensor([4,5,6], dtype=torch.float)\n","print(torch.add(a,b).sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ah5MdCqGDDzx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"YuDM9xfmDDzx"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"N6xraJgsDDzx"},"source":["## Dot products\n","A <a href='https://en.wikipedia.org/wiki/Dot_product'>dot product</a> is the sum of the products of the corresponding entries of two 1D tensors. If the tensors are both vectors, the dot product is given as:<br>\n","\n","$\\begin{bmatrix} a & b & c \\end{bmatrix} \\;\\cdot\\; \\begin{bmatrix} d & e & f \\end{bmatrix} = ad + be + cf$\n","\n","If the tensors include a column vector, then the dot product is the sum of the result of the multiplied matrices. For example:<br>\n","$\\begin{bmatrix} a & b & c \\end{bmatrix} \\;\\cdot\\; \\begin{bmatrix} d \\\\ e \\\\ f \\end{bmatrix} = ad + be + cf$<br><br>\n","Dot products can be expressed as <a href='https://pytorch.org/docs/stable/torch.html#torch.dot'><strong><tt>torch.dot(a,b)</tt></strong></a> or `a.dot(b)` or `b.dot(a)`"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"4X6ZnBcNDDzx","outputId":"2900dad2-31de-43fc-c1e0-57638c77b697"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([ 4., 10., 18.])\n","\n","tensor(32.)\n"]}],"source":["a = torch.tensor([1,2,3], dtype=torch.float)\n","b = torch.tensor([4,5,6], dtype=torch.float)\n","print(a.mul(b)) # for reference\n","print()\n","print(a.dot(b))"]},{"cell_type":"markdown","metadata":{"id":"xtbDn7UJDDzx"},"source":["<div class=\"alert alert-info\"><strong>NOTE:</strong> There's a slight difference between <tt>torch.dot()</tt> and <tt>numpy.dot()</tt>. While <tt>torch.dot()</tt> only accepts 1D arguments and returns a dot product, <tt>numpy.dot()</tt> also accepts 2D arguments and performs matrix multiplication. We show matrix multiplication below.</div>"]},{"cell_type":"markdown","metadata":{"id":"OdWiicEyDDzy"},"source":["## Matrix multiplication\n","2D <a href='https://en.wikipedia.org/wiki/Matrix_multiplication'>Matrix multiplication</a> is possible when the number of columns in tensor <strong><tt>A</tt></strong> matches the number of rows in tensor <strong><tt>B</tt></strong>. In this case, the product of tensor <strong><tt>A</tt></strong> with size $(x,y)$ and tensor <strong><tt>B</tt></strong> with size $(y,z)$ results in a tensor of size $(x,z)$\n","<div>\n","<div align=\"left\"><img src='../Images/Matrix_multiplication_diagram.png' align=\"left\"><br><br>\n","\n","$\\begin{bmatrix} a & b & c \\\\\n","d & e & f \\end{bmatrix} \\;\\times\\; \\begin{bmatrix} m & n \\\\ p & q \\\\ r & s \\end{bmatrix} = \\begin{bmatrix} (am+bp+cr) & (an+bq+cs) \\\\\n","(dm+ep+fr) & (dn+eq+fs) \\end{bmatrix}$</div></div>\n","\n","<div style=\"clear:both\">Image source: <a href='https://commons.wikimedia.org/wiki/File:Matrix_multiplication_diagram_2.svg'>https://commons.wikimedia.org/wiki/File:Matrix_multiplication_diagram_2.svg</a></div>\n","\n","Matrix multiplication can be computed using <a href='https://pytorch.org/docs/stable/torch.html#torch.mm'><strong><tt>torch.mm(a,b)</tt></strong></a> or `a.mm(b)` or `a @ b`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NoA8qUP0DDzy","outputId":"c4a18217-3b48-471c-d31f-fcc26f67518f"},"outputs":[{"name":"stdout","output_type":"stream","text":["a:  torch.Size([2, 3])\n","b:  torch.Size([3, 2])\n","a x b:  torch.Size([2, 2])\n"]}],"source":["a = torch.tensor([[0,2,4],[1,3,5]], dtype=torch.float)\n","b = torch.tensor([[6,7],[8,9],[10,11]], dtype=torch.float)\n","\n","print('a: ',a.size())\n","print('b: ',b.size())\n","print('a x b: ',torch.mm(a,b).size())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ed0hlnbwDDzy","outputId":"3a717639-8923-4f6c-eab1-460f8a4c836b"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[56., 62.],\n","        [80., 89.]])\n"]}],"source":["print(torch.mm(a,b))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xkPcXqk2DDzy","outputId":"816a466b-d292-4fc2-f27a-fc5ce2c13d03"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[56., 62.],\n","        [80., 89.]])\n"]}],"source":["print(a.mm(b))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZhF-B-7ADDzz","outputId":"eeaaae45-2faa-4c6c-9ac8-6296506212f8"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[56., 62.],\n","        [80., 89.]])\n"]}],"source":["print(a @ b)"]},{"cell_type":"markdown","metadata":{"id":"ALNankDDDDzz"},"source":["### Matrix multiplication with broadcasting\n","Matrix multiplication that involves <a href='https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics'>broadcasting</a> can be computed using <a href='https://pytorch.org/docs/stable/torch.html#torch.matmul'><strong><tt>torch.matmul(a,b)</tt></strong></a> or `a.matmul(b)` or `a @ b`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kBIe1vvXDDzz","outputId":"9df180cf-e41d-4339-d1e2-0f367e352066"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 3, 5])\n"]}],"source":["t1 = torch.randn(2, 3, 4)\n","t2 = torch.randn(4, 5)\n","\n","print(torch.matmul(t1, t2).size())"]},{"cell_type":"markdown","metadata":{"id":"6lPG1vOWDDzz"},"source":["However, the same operation raises a <tt><strong>RuntimeError</strong></tt> with <tt>torch.mm()</tt>:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Si6Qx_PqDDz0","outputId":"0a56948f-b6bc-4f15-8b3d-5793d65d752a"},"outputs":[{"ename":"RuntimeError","evalue":"matrices expected, got 3D, 2D tensors at ..\\aten\\src\\TH/generic/THTensorMath.cpp:956","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m<ipython-input-24-edaac219da2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m: matrices expected, got 3D, 2D tensors at ..\\aten\\src\\TH/generic/THTensorMath.cpp:956"]}],"source":["print(torch.mm(t1, t2).size())"]},{"cell_type":"markdown","metadata":{"id":"6G7Z8Ht6DDz0"},"source":["___\n","# Advanced operations"]},{"cell_type":"markdown","metadata":{"id":"HgB9AFyZDDz0"},"source":["## L2 or Euclidian Norm\n","See <a href='https://pytorch.org/docs/stable/torch.html#torch.norm'><strong><tt>torch.norm()</tt></strong></a>\n","\n","The <a href='https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm'>Euclidian Norm</a> gives the vector norm of $x$ where $x=(x_1,x_2,...,x_n)$.<br>\n","It is calculated as<br>\n","\n","${\\displaystyle \\left\\|{\\boldsymbol {x}}\\right\\|_{2}:={\\sqrt {x_{1}^{2}+\\cdots +x_{n}^{2}}}}$\n","\n","\n","When applied to a matrix, <tt>torch.norm()</tt> returns the <a href='https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm'>Frobenius norm</a> by default."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W6DAnmHmDDz0","outputId":"d24ef538-08b9-4de1-8d46-c3194ffdce3a"},"outputs":[{"data":{"text/plain":["tensor(17.)"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["x = torch.tensor([2.,5.,8.,14.])\n","x.norm()"]},{"cell_type":"markdown","metadata":{"id":"68jqftjSDDz0"},"source":["## Number of elements\n","See <a href='https://pytorch.org/docs/stable/torch.html#torch.numel'><strong><tt>torch.numel()</tt></strong></a>\n","\n","Returns the number of elements in a tensor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A4G6zpaLDDz1","outputId":"4239b187-5e55-4088-b7b2-0f21826547d9"},"outputs":[{"data":{"text/plain":["21"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["x = torch.ones(3,7)\n","x.numel()"]},{"cell_type":"markdown","metadata":{"id":"MIPkEjBODDz1"},"source":["This can be useful in certain calculations like Mean Squared Error:<br>\n","<tt>\n","def mse(t1, t2):<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;diff = t1 - t2<br>\n","    &nbsp;&nbsp;&nbsp;&nbsp;return torch.sum(diff * diff) / diff<strong>.numel()</strong></tt>"]},{"cell_type":"markdown","metadata":{"id":"RLlhdvggDDz1"},"source":["## Great work!"]}]}